# ğŸ”’ PII Redaction Dataset Generator  
### Synthetic PII Redaction Pipeline for Fine-Tuning Small LLMs (e.g., Gemma 3 270M)

This repository provides a complete end-to-end pipeline for generating a high-quality synthetic dataset for structured PII redaction, optimized for training compact language models.

It includes:

- Clean canonical base examples  
- A hybrid Regex + Noise Mutation Engine  
- Optional teacher LLM (ChatGPT 5.1) augmentation  
- Dataset balancing tools  
- Schema validation  
- Training-ready JSONL output  

The system aligns with Distil-PII style structured output.

---

## ğŸ“ Project Structure

```
pii_pipeline/
â”‚
â”œâ”€â”€ config.py
â”œâ”€â”€ schemas.py
â”œâ”€â”€ utils.py
â”‚
â”œâ”€â”€ pii_mutation_engine_v2.py
â”œâ”€â”€ teacher_prompts.py
â”œâ”€â”€ teacher_api.py
â”‚
â”œâ”€â”€ dataset_generator.py
â”œâ”€â”€ run_pipeline.py
â”‚
â”œâ”€â”€ balance_dataset.py
â”œâ”€â”€ validate_dataset.py
â”‚
â”œâ”€â”€ clean_samples/
â”‚   â””â”€â”€ base_clean_samples.json
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ temp/
â”‚   â””â”€â”€ final_dataset/
â”‚       â”œâ”€â”€ pii_training_dataset.jsonl
â”‚       â”œâ”€â”€ pii_training_dataset.balanced.jsonl
â”‚       â””â”€â”€ logs/
â”‚
â””â”€â”€ README.md
```

---

## ğŸš€ Overview

This pipeline generates a training-ready JSONL dataset for fine-tuning models to produce structured PII redaction output with mandatory fields:

- redacted_text  
- entities â†’ value, replacement_token, reason

### Features:

- Clean base examples  
- Synthetic corruption engine  
- Teacher LLM augmentation  
- Balancing by PII type  
- Schema validation  
- High volume synthetic dataset generation  

---

## ğŸ”§ Installation

```
git clone <repo>
cd pii_pipeline
pip install -r requirements.txt
```

Typical dependencies:

```
jsonlines
regex
python-dotenv
tqdm
openai
```

---

## ğŸ§  Usage

### 1ï¸âƒ£ Prepare clean base samples

Edit:

```
clean_samples/base_clean_samples.json
```

Each looks like:

```json
{
  "id": "sample_001",
  "question": "Redact provided text...",
  "context": "Hi, I'm John Smith...",
  "answer": {
    "redacted_text": "Hi, I'm [PERSON]...",
    "entities": [
      { "value": "John Smith", "replacement_token": "[PERSON]", "reason": "person name" }
    ]
  }
}
```

---

### 2ï¸âƒ£ Run the full pipeline

```
python run_pipeline.py
```

This generates:

```
outputs/final_dataset/pii_training_dataset.jsonl
```

---

### 3ï¸âƒ£ Balance the dataset

```
python balance_dataset.py outputs/final_dataset/pii_training_dataset.jsonl                           outputs/final_dataset/pii_training_dataset.balanced.jsonl
```

---

### 4ï¸âƒ£ Validate

```
python validate_dataset.py outputs/final_dataset/pii_training_dataset.balanced.jsonl
```

---

## ğŸ“¦ Output Format

Each line in the JSONL dataset follows this schema:

```json
{
  "id": "uuid-or-string",
  "question": "Redact provided text...",
  "context": "Noisy user text",
  "answer": {
    "redacted_text": "Clean output with tokens",
    "entities": [
      {
        "value": "original snippet",
        "replacement_token": "[TOKEN]",
        "reason": "why it was redacted"
      }
    ]
  }
}
```

---

## ğŸ‹ï¸ Fine-Tuning (Gemma 270M)

Recommended hyperparameters:

| Setting | Value |
|--------|-------|
| LR | 2e-4 |
| Scheduler | cosine |
| Warmup | 3% |
| Weight decay | 0.1 |
| Batch size | 64â€“128 |
| Epochs | 3â€“5 |
| Max seq len | 1024 |
| Gradient clipping | 1.0 |

---

## ğŸ“Š Validation Metrics

- Entity-level precision & recall  
- Replacement-token correctness  
- Redacted-text equality  
- PII-class distribution  

---

## ğŸ¤ Contributing

You can contribute by:

- Adding clean samples  
- Expanding mutation rules  
- Improving teacher prompts  
- Adding domain-specific PII types  

---

## ğŸ§© Roadmap

- [x] Regex mutation engine  
- [x] Teacher LLM augmentation  
- [x] Dataset balancer  
- [x] Validator  
- [ ] Multilingual PII  
- [ ] OCR noise simulation  
- [ ] Domain-specific extensions  

---

## ğŸ›¡ License

MIT License
